universe = vanilla
+WantFlocking = true

# the script that will be run when the job starts
executable = run.sh

# include the cluster id and process id that are set at runtime
log = $(Cluster)_$(Process).log
error = $(Cluster)_$(Process).err
output = $(Cluster)_$(Process).out

# these files (code, data, args) get transferred from the submit node to the server on which the program is executing
transfer_input_files = Miniconda3-latest-Linux-x86_64.sh, libc.tar.gz, train.py, SingleType111Loops.zip, VGG_ILSVRC_16_layers_fc_reduced.h5, models.zip, keras_layers.zip, data_generator.zip, ssd_encoder_decoder.zip, bounding_box_utils.zip, keras_loss_function.zip
should_transfer_files = YES
when_to_transfer_output = ON_EXIT

# specify the resources required
request_cpus = 1
request_gpus = 1
request_memory = 2GB
request_disk = 5GB

requirements = ((OpSysMajorVer==6) || (OpSysMajorVer==7)) && (TARGET.CUDADeviceName=="GeForce GTX 1080")

# Copy environment variables that are set dynamically by HTCondor
environment = "cluster=$(Cluster) process=$(Process) runningon=$$(Name)"
getenv=true

# this is how many jobs to queue
queue 1

#END
